{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koQ0t3yTBPAg"
   },
   "source": [
    "# DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ikp82jvGpDK4"
   },
   "source": [
    "## PART 1 : MODIFIED VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "Si9g06HzCJO_",
    "outputId": "5a20e81d-ad92-4a0c-f273-983e25d934ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumyadeep/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "full_model = VGG16(weights='imagenet', input_shape=(None, None, 3), include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGdd_0WZoF2e"
   },
   "outputs": [],
   "source": [
    "layers = [ l for l in full_model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL_6E6vRphBu"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "x = layers[0].output\n",
    "to_avoid = [3, 6, 10, 19, 20, 21]\n",
    "for i in range(1, len(layers)):\n",
    "  if ( i in to_avoid ):\n",
    "    continue\n",
    "  layers[i].trainable = False\n",
    "  x = layers[i](x)\n",
    "\n",
    "model = Model(inputs = [layers[0].input], outputs = [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "colab_type": "code",
    "id": "Rt_0-r4nDKPE",
    "outputId": "0cca9801-f247-4669-d870-5c38fcadfc8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vBtdc1XWgkP"
   },
   "source": [
    "## PART 2 : STORING FEATURE MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "G2pRb7b93kxu",
    "outputId": "9c196254-1aa1-4174-91c3-07a000235963",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('image_0032.jpg', 'already done')\n",
      "('image_0064.jpg', 'already done')\n",
      "('image_0045.jpg', 'already done')\n",
      "('image_0008.jpg', 'already done')\n",
      "('image_0010.jpg', 'already done')\n",
      "('image_0040.jpg', 'already done')\n",
      "('image_0061.jpg', 'already done')\n",
      "('image_0093.jpg', 'already done')\n",
      "('image_0103.jpg', 'already done')\n",
      "('image_0017.jpg', 'already done')\n",
      "('image_0020.jpg', 'already done')\n",
      "('image_0070.jpg', 'already done')\n",
      "('image_0079.jpg', 'already done')\n",
      "('image_0022.jpg', 'already done')\n",
      "('image_0082.jpg', 'already done')\n",
      "('image_0125.jpg', 'already done')\n",
      "('image_0037.jpg', 'already done')\n",
      "('image_0123.jpg', 'already done')\n",
      "('image_0025.jpg', 'already done')\n",
      "('image_0004.jpg', 'already done')\n",
      "('image_0120.jpg', 'already done')\n",
      "('image_0094.jpg', 'already done')\n",
      "('image_0018.jpg', 'already done')\n",
      "('image_0091.jpg', 'already done')\n",
      "('image_0099.jpg', 'already done')\n",
      "('image_0109.jpg', 'already done')\n",
      "('image_0009.jpg', 'already done')\n",
      "('image_0111.jpg', 'already done')\n",
      "('image_0121.jpg', 'already done')\n",
      "('image_0052.jpg', 'already done')\n",
      "('image_0087.jpg', 'already done')\n",
      "('image_0119.jpg', 'already done')\n",
      "('image_0007.jpg', 'already done')\n",
      "('image_0105.jpg', 'already done')\n",
      "('image_0108.jpg', 'already done')\n",
      "('image_0013.jpg', 'already done')\n",
      "('image_0011.jpg', 'already done')\n",
      "('image_0033.jpg', 'already done')\n",
      "('image_0054.jpg', 'already done')\n",
      "('image_0047.jpg', 'already done')\n",
      "('image_0027.jpg', 'already done')\n",
      "('image_0056.jpg', 'already done')\n",
      "('image_0058.jpg', 'already done')\n",
      "('image_0065.jpg', 'already done')\n",
      "('image_0078.jpg', 'already done')\n",
      "('image_0107.jpg', 'already done')\n",
      "('image_0066.jpg', 'already done')\n",
      "('image_0081.jpg', 'already done')\n",
      "('image_0101.jpg', 'already done')\n",
      "('image_0053.jpg', 'already done')\n",
      "('image_0086.jpg', 'already done')\n",
      "('image_0060.jpg', 'already done')\n",
      "('image_0106.jpg', 'already done')\n",
      "('image_0083.jpg', 'already done')\n",
      "('image_0046.jpg', 'already done')\n",
      "('image_0002.jpg', 'already done')\n",
      "('image_0075.jpg', 'already done')\n",
      "('image_0122.jpg', 'already done')\n",
      "('image_0112.jpg', 'already done')\n",
      "('image_0063.jpg', 'already done')\n",
      "('image_0089.jpg', 'already done')\n",
      "('image_0095.jpg', 'already done')\n",
      "('image_0088.jpg', 'already done')\n",
      "('image_0080.jpg', 'already done')\n",
      "('image_0005.jpg', 'already done')\n",
      "('image_0076.jpg', 'already done')\n",
      "('image_0055.jpg', 'already done')\n",
      "('image_0024.jpg', 'already done')\n",
      "('image_0015.jpg', 'already done')\n",
      "('image_0012.jpg', 'already done')\n",
      "('image_0097.jpg', 'already done')\n",
      "('image_0044.jpg', 'already done')\n",
      "('image_0001.jpg', 'already done')\n",
      "('image_0039.jpg', 'already done')\n",
      "('image_0042.jpg', 'already done')\n",
      "('image_0118.jpg', 'already done')\n",
      "('image_0085.jpg', 'already done')\n",
      "('image_0034.jpg', 'already done')\n",
      "('image_0115.jpg', 'already done')\n",
      "('image_0057.jpg', 'already done')\n",
      "('image_0110.jpg', 'already done')\n",
      "('image_0071.jpg', 'already done')\n",
      "('image_0100.jpg', 'already done')\n",
      "('image_0092.jpg', 'already done')\n",
      "('image_0036.jpg', 'already done')\n",
      "('image_0073.jpg', 'already done')\n",
      "('image_0006.jpg', 'already done')\n",
      "('image_0029.jpg', 'already done')\n",
      "('image_0051.jpg', 'already done')\n",
      "('image_0074.jpg', 'already done')\n",
      "('image_0072.jpg', 'already done')\n",
      "('image_0043.jpg', 'already done')\n",
      "('image_0049.jpg', 'already done')\n",
      "('image_0084.jpg', 'already done')\n",
      "('image_0062.jpg', 'already done')\n",
      "('image_0026.jpg', 'already done')\n",
      "('image_0090.jpg', 'already done')\n",
      "('image_0048.jpg', 'already done')\n",
      "('image_0035.jpg', 'already done')\n",
      "('image_0102.jpg', 'already done')\n",
      "('image_0068.jpg', 'already done')\n",
      "('image_0021.jpg', 'already done')\n",
      "('image_0117.jpg', 'already done')\n",
      "('image_0003.jpg', 'already done')\n",
      "('image_0059.jpg', 'already done')\n",
      "('image_0038.jpg', 'already done')\n",
      "('image_0069.jpg', 'already done')\n",
      "('image_0126.jpg', 'already done')\n",
      "('image_0041.jpg', 'already done')\n",
      "('image_0124.jpg', 'already done')\n",
      "('image_0098.jpg', 'already done')\n",
      "('image_0116.jpg', 'already done')\n",
      "('image_0077.jpg', 'already done')\n",
      "('image_0050.jpg', 'already done')\n",
      "('image_0030.jpg', 'already done')\n",
      "('image_0114.jpg', 'already done')\n",
      "('image_0096.jpg', 'already done')\n",
      "('image_0016.jpg', 'already done')\n",
      "('image_0104.jpg', 'already done')\n",
      "('image_0113.jpg', 'already done')\n",
      "('image_0014.jpg', 'already done')\n",
      "('image_0028.jpg', 'already done')\n",
      "('image_0019.jpg', 'already done')\n",
      "('image_0031.jpg', 'already done')\n",
      "('image_0023.jpg', 'already done')\n",
      "('image_0067.jpg', 'already done')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "anchors = [(12,12,2,2),(19,20,3,4),(22,22,5,5),(34,35,6,7),(42,42,8,8),(49,50,9,10)]\n",
    "\n",
    "dataset_folder = 'DATASET'\n",
    "feature_folder = 'FEATURE_MAPS'\n",
    "\n",
    "for im in os.listdir(dataset_folder):\n",
    "    if 'image_' in im:\n",
    "        output_name = os.path.join(feature_folder, 'image_' + im.split('.')[0].split('_')[1] + '.h5')\n",
    "        if os.path.isfile(output_name):\n",
    "            print(im, 'already done')\n",
    "            continue\n",
    "        image = cv2.imread(os.path.join(dataset_folder, im))\n",
    "        image = image.reshape(1, *image.shape)\n",
    "        print('Processing', im, image.shape)\n",
    "        start = time.time()\n",
    "        result = model.predict(image)\n",
    "        end = time.time()\n",
    "        print(result.shape, \"Time taken\", end-start)\n",
    "        with h5py.File(output_name, 'w') as f:\n",
    "            f.create_dataset(\"default\", data=result, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 : STORING ANCHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [01:01<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "anchors = [(12,12,2,2),(19,20,3,4),(22,22,5,5),(34,35,6,7),(42,42,8,8),(49,50,9,10)]\n",
    "\n",
    "feature_folder = 'FEATURE_MAPS'\n",
    "anchor_folder = 'ANCHORS'\n",
    "\n",
    "for fm in tqdm(os.listdir(feature_folder)):\n",
    "    feature_map = h5py.File(feature_folder+\"/\"+fm,'r+')\n",
    "    feature_map = np.array(feature_map.get('default'))\n",
    "    [ n , n_i, n_j, n_k ] = feature_map.shape\n",
    "    output_name = anchor_folder+'/'+fm.split('.')[0]+'.h5'\n",
    "    if os.path.isfile(output_name):\n",
    "        continue\n",
    "    with h5py.File(output_name,'w') as hf:\n",
    "        cnt = 0    \n",
    "        for w_l, w_r, h_l, h_r in anchors:\n",
    "            for i in range( w_l, n_i-w_r, 10):\n",
    "                for j in range( h_l, n_j-h_r, 10):                             \n",
    "                    anchor = feature_map[:,i-w_l:i+w_r,j-h_l:j+h_r,:]                \n",
    "                    hf.create_dataset('anchor_'+str(cnt), data=anchor, compression=\"gzip\")\n",
    "                    cnt += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "END2END2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
